#!/usr/bin/env python3
"""
Script principal pour exÃ©cuter le pipeline de donnÃ©es AIS Tanger Med
Support des fichiers ZIP NOAA et donnÃ©es locales
Usage: python scripts/run_pipeline.py [--url URL] [--local-file FILE] [--noaa-year YEAR] [options]
"""

import argparse
import logging
import sys
import os
from pathlib import Path
import time
import pandas as pd

# Ajouter src au path
sys.path.append(str(Path(__file__).parent.parent / "src"))

from src.config import Config
from src.ingestion.data_loader import AISDataLoader
from src.transformation.data_processor import AISDataProcessor
from src.storage.database import DatabaseManager
from src.analytics.statistics import StatisticsGenerator

# Configuration du logging avec format dÃ©taillÃ©
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('logs/pipeline.log', mode='a') if os.path.exists('logs') else logging.NullHandler()
    ]
)
logger = logging.getLogger(__name__)

def parse_arguments():
    """Parse les arguments de ligne de commande"""
    parser = argparse.ArgumentParser(
        description='Pipeline de donnÃ©es AIS Tanger Med avec support NOAA',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  # Fichier local
  python scripts/run_pipeline.py --local-file sample_data/sample_ais.csv
  
  # DonnÃ©es NOAA 2024
  python scripts/run_pipeline.py --noaa-year 2024 --noaa-zone "01_01"
  
  # URL directe
  python scripts/run_pipeline.py --url "https://coast.noaa.gov/htdata/CMSP/AISDataHandler/2024/AIS_2024_01_01.zip"
  
  # Fichier ZIP local
  python scripts/run_pipeline.py --local-file data/AIS_2024_01_01.zip
  
  # GÃ©nÃ©rer seulement les statistiques
  python scripts/run_pipeline.py --generate-stats-only
        """
    )
    
    # Groupe des sources de donnÃ©es
    source_group = parser.add_mutually_exclusive_group()
    source_group.add_argument(
        '--url', 
        help='URL des donnÃ©es AIS Ã  tÃ©lÃ©charger (CSV ou ZIP)'
    )
    source_group.add_argument(
        '--local-file', 
        help='Fichier local de donnÃ©es AIS (CSV ou ZIP)'
    )
    source_group.add_argument(
        '--noaa-year',
        help='AnnÃ©e des donnÃ©es NOAA Ã  tÃ©lÃ©charger (ex: 2024)'
    )
    
    # Options NOAA spÃ©cifiques
    parser.add_argument(
        '--noaa-zone',
        help='Zone NOAA spÃ©cifique (ex: "01_01", "02_15")'
    )
    
    # Options de traitement
    parser.add_argument(
        '--skip-download', 
        action='store_true',
        help='Ignorer le tÃ©lÃ©chargement (utiliser un fichier existant)'
    )
    parser.add_argument(
        '--skip-processing', 
        action='store_true',
        help='Ignorer le traitement (charger les donnÃ©es brutes)'
    )
    parser.add_argument(
        '--skip-stats', 
        action='store_true',
        help='Ignorer la gÃ©nÃ©ration des statistiques'
    )
    parser.add_argument(
        '--generate-stats-only', 
        action='store_true',
        help='GÃ©nÃ©rer uniquement les statistiques (sans traitement de donnÃ©es)'
    )
    
    # Options de configuration
    parser.add_argument(
        '--batch-size', 
        type=int, 
        default=1000,
        help='Taille des lots pour l\'insertion en base (dÃ©faut: 1000)'
    )
    parser.add_argument(
        '--max-records', 
        type=int,
        help='Nombre maximum d\'enregistrements Ã  traiter'
    )
    parser.add_argument(
        '--extract-dir',
        help='RÃ©pertoire d\'extraction pour les fichiers ZIP'
    )
    parser.add_argument(
        '--verbose', '-v', 
        action='store_true',
        help='Mode verbeux (logs dÃ©taillÃ©s)'
    )
    
    return parser.parse_args()

def setup_logging(verbose: bool):
    """Configure le niveau de logging"""
    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.info("Mode verbeux activÃ©")

def validate_file(file_path: str) -> bool:
    """Valide l'existence et la lisibilitÃ© d'un fichier"""
    if not os.path.exists(file_path):
        logger.error(f"âŒ Fichier introuvable: {file_path}")
        return False
    
    if not os.path.isfile(file_path):
        logger.error(f"âŒ Le chemin n'est pas un fichier: {file_path}")
        return False
    
    if not os.access(file_path, os.R_OK):
        logger.error(f"âŒ Fichier non lisible: {file_path}")
        return False
    
    # VÃ©rifier la taille du fichier
    file_size = os.path.getsize(file_path)
    logger.info(f"ğŸ“Š Taille du fichier: {file_size:,} bytes ({file_size / (1024*1024):.1f} MB)")
    
    if file_size == 0:
        logger.error(f"âŒ Fichier vide: {file_path}")
        return False
    
    return True

def build_noaa_url(year: str, zone: str = None) -> str:
    """Construit l'URL pour les donnÃ©es NOAA"""
    base_url = f"https://coast.noaa.gov/htdata/CMSP/AISDataHandler/{year}/"
    
    if zone:
        filename = f"AIS_{year}_{zone}.zip"
    else:
        # Fichier par dÃ©faut - premier de l'annÃ©e
        filename = f"AIS_{year}_01_01.zip"
    
    return f"{base_url}{filename}"

def ingest_data(args, config: Config) -> str:
    """Ã‰tape d'ingestion des donnÃ©es avec support ZIP"""
    logger.info("ğŸš€ Ã‰TAPE 1: Ingestion des donnÃ©es")
    logger.info("-" * 40)
    
    loader = AISDataLoader(config)
    
    if args.local_file:
        file_path = args.local_file
        logger.info(f"ğŸ“ Utilisation du fichier local: {file_path}")
        
        if not validate_file(file_path):
            raise ValueError(f"Fichier invalide: {file_path}")
            
    elif args.noaa_year:
        # TÃ©lÃ©chargement NOAA
        url = build_noaa_url(args.noaa_year, args.noaa_zone)
        filename = os.path.basename(url)
        file_path = f"data/{filename}"
        
        os.makedirs("data", exist_ok=True)
        
        logger.info(f"ğŸŒŠ TÃ©lÃ©chargement des donnÃ©es NOAA {args.noaa_year}")
        logger.info(f"Zone: {args.noaa_zone if args.noaa_zone else 'par dÃ©faut (01_01)'}")
        logger.info(f"URL: {url}")
        
        start_time = time.time()
        
        if not loader.download_ais_data(url, file_path):
            raise Exception(f"Ã‰chec du tÃ©lÃ©chargement NOAA depuis {url}")
        
        download_time = time.time() - start_time
        logger.info(f"âœ… TÃ©lÃ©chargement NOAA terminÃ© en {download_time:.1f}s")
        
    elif args.url and not args.skip_download:
        # URL directe
        file_path = f"data/{os.path.basename(args.url)}"
        os.makedirs("data", exist_ok=True)
        
        logger.info(f"ğŸŒ TÃ©lÃ©chargement depuis: {args.url}")
        start_time = time.time()
        
        if not loader.download_ais_data(args.url, file_path):
            raise Exception("Ã‰chec du tÃ©lÃ©chargement")
        
        download_time = time.time() - start_time
        logger.info(f"âœ… TÃ©lÃ©chargement terminÃ© en {download_time:.1f}s")
        
    else:
        raise ValueError("Veuillez spÃ©cifier --url, --local-file ou --noaa-year")
    
    return file_path

def process_data(file_path: str, args, config: Config):
    """Ã‰tape de traitement des donnÃ©es avec support ZIP"""
    logger.info("ğŸ”„ Ã‰TAPE 2: Traitement des donnÃ©es")
    logger.info("-" * 40)
    
    # Chargement (avec support ZIP automatique)
    loader = AISDataLoader(config)
    logger.info(f"ğŸ“– Chargement du fichier: {file_path}")
    
    # Indiquer si c'est un ZIP
    if file_path.lower().endswith('.zip'):
        logger.info("ğŸ—œï¸ Fichier ZIP dÃ©tectÃ© - extraction automatique")
    
    start_time = time.time()
    
    df = loader.load_csv_data(file_path)
    if df is None:
        raise Exception("Ã‰chec du chargement des donnÃ©es")
    
    
    logger.info(df.head())

    load_time = time.time() - start_time
    logger.info(f"âœ… DonnÃ©es chargÃ©es: {len(df):,} enregistrements en {load_time:.1f}s")
    
    # Limitation optionnelle
    if args.max_records and len(df) > args.max_records:
        logger.info(f"ğŸ”’ Limitation Ã  {args.max_records:,} enregistrements")
        df = df.head(args.max_records)
    
    # Transformation
    processor = AISDataProcessor()
    
    if not args.skip_processing:
        logger.info("ğŸ§¹ Nettoyage des donnÃ©es...")
        start_time = time.time()
        cleaned_df = processor.clean_data(df)
        clean_time = time.time() - start_time
        
        logger.info(f"âœ… Nettoyage terminÃ©: {len(cleaned_df):,} enregistrements valides en {clean_time:.1f}s")
        logger.info(cleaned_df.head())

        
        # Calcul des mÃ©triques par navire
        logger.info("ğŸ“Š Calcul des mÃ©triques par navire...")
        start_time = time.time()
        vessel_metrics = processor.calculate_vessel_metrics(cleaned_df)
        metrics_time = time.time() - start_time
        
        logger.info(f"âœ… MÃ©triques calculÃ©es pour {len(vessel_metrics):,} navires en {metrics_time:.1f}s")
        logger.info(vessel_metrics.head())
    else:
        logger.info("â­ï¸ Nettoyage ignorÃ© - utilisation des donnÃ©es brutes")
        cleaned_df = df
        vessel_metrics = pd.DataFrame()  # Vide si pas de traitement
    
    return cleaned_df, vessel_metrics

def store_data(cleaned_df, vessel_metrics, args, config: Config):
    """Ã‰tape de stockage en base de donnÃ©es"""
    logger.info("ğŸ’¾ Ã‰TAPE 3: Stockage en base de donnÃ©es")
    logger.info("-" * 40)
    
    db_manager = DatabaseManager(config.database_url)
    
    # CrÃ©er les tables si nÃ©cessaire
    logger.info("ğŸ—ï¸ VÃ©rification/crÃ©ation des tables...")
    db_manager.create_tables()
    
    # Sauvegarde des donnÃ©es AIS
    if len(cleaned_df) > 0:
        logger.info(f"ğŸ’¾ Sauvegarde de {len(cleaned_df):,} enregistrements AIS...")
        start_time = time.time()
        
        # Sauvegarde par lots si le dataset est gros
        batch_size = args.batch_size
        if len(cleaned_df) > batch_size:
            logger.info(f"Sauvegarde par lots de {batch_size:,}")
            for i in range(0, len(cleaned_df), batch_size):
                batch = cleaned_df.iloc[i:i+batch_size]
                db_manager.save_ais_data(batch)
                logger.info(f"  Lot {i//batch_size + 1}: {len(batch):,} enregistrements")
        else:
            db_manager.save_ais_data(cleaned_df)
        
        save_time = time.time() - start_time
        logger.info(f"âœ… DonnÃ©es AIS sauvegardÃ©es en {save_time:.1f}s")
    
    # Sauvegarde des mÃ©triques
    if len(vessel_metrics) > 0:
        logger.info(f"ğŸ“Š Sauvegarde de {len(vessel_metrics):,} mÃ©triques de navires...")
        start_time = time.time()
        db_manager.save_vessel_metrics(vessel_metrics)
        metrics_save_time = time.time() - start_time
        logger.info(f"âœ… MÃ©triques sauvegardÃ©es en {metrics_save_time:.1f}s")

def generate_statistics(config: Config):
    """Ã‰tape de gÃ©nÃ©ration des statistiques"""
    logger.info("ğŸ“ˆ Ã‰TAPE 4: GÃ©nÃ©ration des statistiques")
    logger.info("-" * 40)
    
    generator = StatisticsGenerator(config)
    start_time = time.time()
    
    report = generator.generate_comprehensive_report()
    stats_time = time.time() - start_time
    
    logger.info(f"âœ… Statistiques gÃ©nÃ©rÃ©es en {stats_time:.1f}s")
    
    # Afficher un rÃ©sumÃ©
    time_stats = report.get('time_analysis', {})
    logger.info(f"ğŸ“Š RÃ©sumÃ©: {time_stats.get('total_moving_time', 0):.1f}h en mouvement, "
               f"{time_stats.get('total_dock_time', 0):.1f}h Ã  quai")
    
    return report

def main():
    """Fonction principale d'exÃ©cution du pipeline"""
    logger.info("ğŸš¢ Pipeline de DonnÃ©es AIS Tanger Med")
    logger.info("=" * 60)
    
    try:
        # Parse des arguments
        args = parse_arguments()
        setup_logging(args.verbose)
        
        # Charger la configuration
        config = Config()
        logger.info(f"Configuration chargÃ©e:")
        logger.info(f"  - Base de donnÃ©es: {config.DB_HOST}:{config.DB_PORT}/{config.DB_NAME}")
        logger.info(f"  - Utilisateur: {config.DB_USER}")
        
        # CrÃ©er les rÃ©pertoires nÃ©cessaires
        os.makedirs("data", exist_ok=True)
        os.makedirs("logs", exist_ok=True)
        
        # Cas spÃ©cial: gÃ©nÃ©ration des statistiques seulement
        if args.generate_stats_only:
            logger.info("ğŸ”„ Mode statistiques uniquement")
            report = generate_statistics(config)
            logger.info("ğŸ‰ Pipeline terminÃ© avec succÃ¨s!")
            return
        
        # Ã‰tape 1: Ingestion
        file_path = ingest_data(args, config)
        
        # Ã‰tape 2: Traitement
        cleaned_df, vessel_metrics = process_data(file_path, args, config)
        
        # Ã‰tape 3: Stockage
        store_data(cleaned_df, vessel_metrics, args, config)
        
        # Ã‰tape 4: Statistiques (optionnel)
        if not args.skip_stats:
            report = generate_statistics(config)
        
        # Nettoyage optionnel des fichiers temporaires
        if args.extract_dir and os.path.exists(args.extract_dir):
            logger.info(f"ğŸ§¹ Nettoyage du rÃ©pertoire temporaire: {args.extract_dir}")
            import shutil
            shutil.rmtree(args.extract_dir)
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ Pipeline exÃ©cutÃ© avec succÃ¨s!")
        logger.info("ğŸ’¡ Prochaines Ã©tapes:")
        logger.info("  - DÃ©marrer l'API: python -m uvicorn src.api.main:app --reload")
        logger.info("  - AccÃ©der au dashboard: http://localhost:8501")
        logger.info("  - Tester l'API: curl http://localhost:8000/statistics")
        
    except KeyboardInterrupt:
        logger.info("âŒ Interruption par l'utilisateur")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ Erreur lors de l'exÃ©cution du pipeline: {e}")
        if args.verbose:
            import traceback
            logger.error(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()